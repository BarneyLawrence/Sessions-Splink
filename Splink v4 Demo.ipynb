{"cells":[{"cell_type":"markdown","source":["# Splink v4 Deduplication Demo\n","\n","This demo shows the steps to deduplicate a sample data set along with examples of visualisations splink provides.\n","\n","The demo code is based on the exampe [Deduplicate 50k rows historical persons](https://moj-analytical-services.github.io/splink/demos/examples/duckdb/deduplicate_50k_synthetic.html) in the official splink documentation with changes by Barney Lawrence to expand on the concepts demonstrated and to adapt the code to run in a Microsoft Fabric or Synapse Analytics environment."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b93edf7b-51f8-4cd8-a8f0-f30806281e2d"},{"cell_type":"markdown","source":["## Install Splink\n","For convenience splink is installed direct via splink.\n","\n","In a production environment splink should be pre-loaded at a defined version number to avoid issues with unexpected version changes. Similarly inside a private network it may not be possible to access the online versions of the library and its dependencies.\n","\n","This is easily done in Fabric with a custom environment and less easily done in Synapse Analytics by extracting the relevant WHL files and manually adding them to the workspace and spark pool."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d61a376-15eb-465d-bb18-8bae469de99a"},{"cell_type":"code","source":["%pip install splink\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0b91aed-1730-46b8-9376-68bf1e8f98f7"},{"cell_type":"markdown","source":["## Setup\n","Load in libraries and some initialisation to create the file paths splink needs to work in a Microsoft environment.\n","\n","Suppressing deprecation warnings as splink sometimes uses older functions and can trigger a lot of warnings in the outputs that we can't do anything about."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21a672f8-6895-46a7-8d92-18915ae4a503"},{"cell_type":"code","source":["from splink import DuckDBAPI, SparkAPI\n","\n","db_api = DuckDBAPI()\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","spark.sparkContext.setCheckpointDir(\"Files/tmp_checkpoints\")\n","\n","import os\n","\n","os.makedirs(\"Files/TempReports\", exist_ok=True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33f658d8-2022-47e7-80a7-6b38be2321e9"},{"cell_type":"markdown","source":["## Sample Data\n","\n","Reading in the historical 50k sample data set included with splink."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dcc74b94-965a-4b96-9dda-3acfacbe04ec"},{"cell_type":"code","source":["from splink import splink_datasets\n","\n","df = splink_datasets.historical_50k\n","\n","df[:1000]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6c50594-040c-4b35-90f3-e66885454ec0"},{"cell_type":"markdown","source":["## Profile Columns\n","This visualisation gives an example of the tools available in splink to support initial exploration of a data set.\n","\n","In this case we generate profiling of a defined set of columns to understand high and low cardinality values in them."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ae5694f-5187-4914-bb38-69aa324af35e"},{"cell_type":"code","source":["from splink.exploratory import profile_columns\n","\n","profile_columns(df, db_api,\n","    column_expressions=[\"first_name\", \"surname\", \"occupation\", \"substr(dob, 1,4)\"], \n","    top_n=10, bottom_n=10\n",")\n","    "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90e6f30e-065c-4ca7-8ef2-7b8888163943"},{"cell_type":"markdown","source":["## Blocking Rules\n","Blocking rules define the blocks of rows that will be compared rather than running a full cartesian product of the data.\n","\n","Blocking rules can be defined within the main settings object but it's neater to produce them separately and this also allows for pre analysis of their impact on performance.\n","\n","The chart generated shows how each rule adds to the cumulative total of distinct pairs compared by the process."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0796e5a7-3581-4e1d-8e6e-0085c9323284"},{"cell_type":"code","source":["from splink import block_on\n","from splink.blocking_analysis import (\n","    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n",")\n","\n","blocking_rules = [\n","    block_on(\"substr(first_name,1,3)\", \"substr(surname,1,4)\"),\n","    block_on(\"surname\", \"dob\"),\n","    block_on(\"first_name\", \"dob\"),\n","    block_on(\"postcode_fake\", \"first_name\"),\n","    block_on(\"postcode_fake\", \"surname\"),\n","    block_on(\"dob\", \"birth_place\"),\n","    block_on(\"substr(postcode_fake,1,3)\", \"dob\"),\n","    block_on(\"substr(postcode_fake,1,3)\", \"first_name\"),\n","    block_on(\"substr(postcode_fake,1,3)\", \"surname\"),\n","    block_on(\"substr(first_name,1,2)\", \"substr(surname,1,2)\", \"substr(dob,1,4)\"),\n","]\n","\n","cumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n","    table_or_tables=df,\n","    blocking_rules=blocking_rules,\n","    db_api=db_api,\n","    link_type=\"dedupe_only\",\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df6259ed-b73a-45f7-b4f9-0f766a88f8cc"},{"cell_type":"markdown","source":["## Comparisons\n","\n","Define the comparisons splink will make between fields within record pairs to assess the probability of a match.\n","\n","Comparisons can use included pre-defined comparison templates,  be custom built from sets of pre-defined levels or coded directly as SQL syntax.\n","\n","Each comparison can all be defined directly in the group of comparisons or be defined as a variable beforehand."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b30a40d-82e4-4410-8e51-fa99c2a0d4d0"},{"cell_type":"code","source":["import splink.comparison_library as cl\n","\n","# Needed to apply term frequencies to first+surname comparison\n","df[\"first_name_surname_concat\"] = df[\"first_name\"] + \" \" + df[\"surname\"]\n","\n","NameComparison = cl.ForenameSurnameComparison(\n","            \"first_name\",\n","            \"surname\",\n","            forename_surname_concat_col_name=\"first_name_surname_concat\",\n","        )\n","\n","comparisons = [\n","        NameComparison,\n","        cl.DateOfBirthComparison(\n","            \"dob\", input_is_string=True\n","        ),\n","        cl.PostcodeComparison(\"postcode_fake\"),\n","        cl.ExactMatch(\"birth_place\").configure(term_frequency_adjustments=True),\n","        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=True),\n","    ]\n","\n","print(NameComparison.get_comparison(\"duckdb\").human_readable_description)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"459e4311-8f98-4361-882f-653996ad99c7"},{"cell_type":"markdown","source":["## Settings\n","\n","Apply blocking rules, comparisons and other settings to define the needed parameters for the linking process.\n","\n","These are then applied to create the linker. In this case we only add a single data set as we are de-duplicating within it rather than matching between multiple data sets."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"126f8e61-60a6-42e1-b6d6-cdfa5355618d"},{"cell_type":"code","source":["from splink import Linker, SettingsCreator\n","\n","settings = SettingsCreator(\n","    link_type= \"dedupe_only\",\n","    blocking_rules_to_generate_predictions=blocking_rules,\n","    comparisons=comparisons,\n","    retain_matching_columns= True,\n","    retain_intermediate_calculation_columns= True,\n",")\n","\n","linker = Linker(df, settings, db_api=db_api)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b6b0c28-65d5-42fb-a68d-4263e827caee"},{"cell_type":"markdown","source":["## Estimate Probability Two Random Records Match\n","\n","Generate a baseline understanding of how many matches are expected within the data by specifying deterministic rules and an approximate proportion of matches they will find."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4d12c14-f432-4558-b827-abdcb0d36051"},{"cell_type":"code","source":["deterministic_rules = [\n","        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n","        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n","        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n","    ]\n","\n","linker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33462431-2684-430f-8827-ac63354f5c5a"},{"cell_type":"markdown","source":["## Estimate U Using Random Sampling\n","As randomly picked record pairs are highly unlikely to form a true match we can use these to calculate the majority of U values."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"01f51c86-626c-4b1e-8f7a-03d6e15be9eb"},{"cell_type":"code","source":["linker.training.estimate_u_using_random_sampling(max_pairs=5e6)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f6bd762-9c26-411e-b9c0-cddf6b609ed3"},{"cell_type":"markdown","source":["## Train M from Label Column\n","If the data contains a label column (any id that uniquely identifies an individual such as National Insurance Number, NHS Number or another shared person id) it can me used to estimate the majority of M values.\n","\n","For demonstration purposes skip this step to get a better demonstration of expectation maximisation."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ccd16d5a-799a-4828-b45b-dcde1a6823db"},{"cell_type":"code","source":["#linker.training.estimate_m_from_label_column(\"cluster\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10c7bb5b-8adf-4529-8314-14bc35ec0e49"},{"cell_type":"markdown","source":["## Train Using Expectation Maximisation\n","\n","M and U values can be trained without label fields by defining blocking rules to generate record pairs and studying the comparison levels for other fields.\n","\n","Multiple attempts are likely needed to gain a full picture. The first block of code below fails to train for name because part of full name is in every blocking rule. THe second code block fixes this by leaving both first name and surname unconstrained.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6826db1d-7d56-4d5e-b570-602dff97916c"},{"cell_type":"code","source":["training_blocking_rule = block_on(\"surname\",\"first_name\",\"dob\")\n","training_session_1 = linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n","\n","training_blocking_rule = block_on(\"surname\",\"first_name\",\"postcode_fake\")\n","training_session_2 = linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n","\n","training_blocking_rule = block_on(\"first_name\",\"postcode_fake\",\"dob\")\n","training_session_3 = linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n","\n","training_blocking_rule = block_on(\"surname\",\"postcode_fake\",\"dob\")\n","training_session_4 = linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49d7226c-88bb-4ca4-8875-1a63e1fda1b5"},{"cell_type":"code","source":["training_blocking_rule = block_on(\"dob\",\"occupation\")\n","training_session_6 = linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4542abe-5716-4c1e-9e18-0b227dadf587"},{"cell_type":"markdown","source":["## Match Weights Chart\n","This visual shows the final trained result giving weightings for each comparison level of every comparison."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cdedb50a-ae7b-49af-a379-72334d035bbe"},{"cell_type":"code","source":["linker.visualisations.match_weights_chart()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9357bfa8-1efc-46cc-aad6-7ed67e52322c"},{"cell_type":"markdown","source":["## Term Frequencies Chart\n","This chart shows details of values where an adjustment to a weighting will be made because the specific field value is either more or less common than average."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74546e2e-94a3-4bec-9b80-3121ce8aadcd"},{"cell_type":"code","source":["linker.visualisations.tf_adjustment_chart(\"occupation\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b9dd996-c22e-4af8-bf00-4dd1ba90b794"},{"cell_type":"markdown","source":["## Predict Results\n","Apply the calculated weightings to all record pairs generated through blocking rules. Threshold prevents low weighted pairs from being included in the output.\n","\n","Results are a simple data frame with input columns plus weighting, probability and other columns related to the matching process."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4cddd607-92e6-4401-8a4a-2cc5f504d09c"},{"cell_type":"code","source":["results = linker.inference.predict(threshold_match_probability=0.5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ba8bb4d-a25b-4ee6-b284-f6e4fe14966a"},{"cell_type":"code","source":["display(results.as_pandas_dataframe(limit=1000))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dc5cd020-d526-483f-a3ea-a310c77acc59"},{"cell_type":"markdown","source":["## Comparison Viewer Dashboard\n","\n","This dashboard shows all combinations of comparison levels observed in the results ordered by weight. For each an example matched pair can be studied.\n","\n","Results can be saved as an html file and shared."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c70f88c9-5aaf-48b2-8593-ceae5abe4db0"},{"cell_type":"code","source":["\n","linker.visualisations.comparison_viewer_dashboard(results, \"Files/TempReports/comparisons.html\", overwrite=True)\n","\n","f = open('Files/TempReports/comparisons.html', 'r')\n","\n","displayHTML(f.read())\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de7fcc48-61db-4e42-a944-15a1f2ff1dbb"},{"cell_type":"markdown","source":["## Generate Clusters\n","\n","Based on weightings, clusters of records considered to represent the same entity can be created grouped by a cluster id."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b01a58e2-1cb9-49ba-b22e-aabfa8beb39e"},{"cell_type":"code","source":["clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(results, threshold_match_probability=0.95)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c6d4dd3-7191-4d95-a567-6736d8512ad4"},{"cell_type":"code","source":["display(clusters.as_pandas_dataframe(limit=1000).sort_values('cluster_id'))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d94c9661-961c-473c-b48b-a470a337acdf"},{"cell_type":"markdown","source":["## Cluster Studio Dashboard\n","This dashboard visualises records within a cluster and the relationships between them."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2458e63-c32d-42a7-b0c1-90f83b396c5e"},{"cell_type":"code","source":["\n","linker.visualisations.cluster_studio_dashboard(\n","    results,\n","    clusters,\n","    \"Files/TempReports/50k_cluster.html\",\n","    sampling_method=\"by_cluster_size\",\n","    overwrite=True,\n",")\n","\n","f = open('Files/TempReports/50k_cluster.html', 'r')\n","\n","displayHTML(f.read())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da0b42ff-2abd-4671-9b26-8f9fe1959b5a"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"e978c1af-27f3-40c8-bee2-e9eb3be5c498","default_lakehouse_name":"Splinkhouse","default_lakehouse_workspace_id":"b9b9d608-a0b2-4d60-8a6a-5b0bc61cb4a7"},"environment":{}}},"nbformat":4,"nbformat_minor":5}